{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad66e1d",
   "metadata": {},
   "source": [
    "## Here 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"[16.22, 84.86, 36.76, 52.43, 69.19, 45.95, 108.11, 50.27, 118.92, 60.0, 134.05, 76.22, 132.97, 104.32, 145.95, 128.11, 151.35, 143.24, 143.78, 164.86, 139.46, 174.59, 151.35, 191.89, 189.19, 223.24, 230.27, 265.41, 235.68, 268.65, 238.92, 282.7, 232.43, 297.84, 223.78, 302.16, 183.78, 290.27, 158.92, 271.89, 105.95, 251.35, 122.16, 272.97, 137.3, 287.03, 149.19, 312.97, 162.16, 338.92, 169.73, 349.73, 174.05, 361.62, 161.08, 396.22, 149.19, 423.24, 139.46, 437.3, 125.41, 448.11, 113.51, 453.51, 104.86, 454.59, 98.38, 444.86, 78.92, 426.49, 55.14, 402.7, 40.0, 387.57, 21.62, 368.11, 1.08, 348.65, 0.0, 136.76, 0.0, 123.78]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.replace('[', '').replace(']', '').split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(map(lambda i: float(i), sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [round(sample[i]/640, 3) if i%2!=0 else round(sample[i]/480, 3) for i in range(len(sample))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = str(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor\n",
    "import torch\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate the float numbers from 0.00 to 1.00 with a step of 0.01\n",
    "float_numbers = np.arange(0.00, 1.001, 0.001)\n",
    "precision = 0.001\n",
    "# Convert the float numbers to strings with two decimal places\n",
    "new_tokens = [f\"{num:.{len(str(precision).split('.')[-1])}f}\" for num in float_numbers]\n",
    "\n",
    "# Assuming 'processor' is an object that has a 'tokenizer' attribute\n",
    "# and that 'tokenizer' has an 'add_tokens' method (common in libraries like Hugging Face Transformers)\n",
    "processor.tokenizer.add_tokens(new_tokens)\n",
    "processor.tokenizer.add_tokens([', '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e271c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.encode('[')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processor.tokenizer.encode(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.encode('[0.034, 0.133, 0.077]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ffe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a8bb5",
   "metadata": {},
   "source": [
    "## Here 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2af96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor\n",
    "import torch\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.apply_chat_template([\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'image'},\n",
    "            {'type': 'text', 'text': 'This is some path.'}\n",
    "        ]\n",
    "    }\n",
    "], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2323737",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_input = processor.apply_chat_template([\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': [\n",
    "            {'type': 'text', 'text': 'You are a helpful assistant.'}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'image'},\n",
    "            {'type': 'text', 'text': 'This is some path.'}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': [\n",
    "            {'type': 'text', 'text': 'This is some answer.'}\n",
    "        ]\n",
    "    }\n",
    "], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71392b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formal_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=formal_input, images=[image], return_tensors=\"pt\", max_length=1000, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7025ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22422f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(inputs['input_ids'], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = inputs.input_ids\n",
    "full_attn_mask = inputs.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba45f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_input = processor.apply_chat_template([\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': [\n",
    "            {'type': 'text', 'text': 'You are a helpful assistant.'}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'image'},\n",
    "            {'type': 'text', 'text': 'This is some path.'}\n",
    "        ]\n",
    "    },\n",
    "], tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bfebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=formal_input, images=[image], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(inputs.input_ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_user = inputs.input_ids\n",
    "sys_user_attn_mask = inputs.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21255038",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf290e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_input_asst = processor.apply_chat_template([\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': [\n",
    "            {'type': 'text', 'text': 'This is some answer.'}\n",
    "        ]\n",
    "    }\n",
    "], tokenize=False)\n",
    "asst_inputs = processor(text=formal_input_asst, images=None, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "asst = asst_inputs.input_ids[:, 1:]\n",
    "asst_attn_mask = asst_inputs.attention_mask[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formal_input_asst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(asst_inputs.input_ids[:, 1:], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(full, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(torch.cat([sys_user, asst], dim=1), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape, torch.cat([sys_user, asst], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5918f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if the full input_ids are equal to the concatenated sys_user and asst inputs\n",
    "concat = torch.cat([sys_user, asst], dim=1)\n",
    "for _ in range(len(full[0])):\n",
    "    if full[0][_].item() != concat[0][_].item():\n",
    "        print(f\"full[0][{_}] = {full[0][_].item()} != concat[0][{_}] = {concat[0][_].item()}\")\n",
    "        print(f\"decode the item: {processor.batch_decode(torch.tensor([full[0][_]]), skip_special_tokens=False)}\")\n",
    "        print(f\"decode the assistant: {processor.batch_decode(torch.tensor([concat[0][_]]), skip_special_tokens=False)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.all(full == torch.cat([sys_user, asst], dim=1)), \"The full input_ids are not equal to the concatenated sys_user and asst inputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_attn_mask.shape, torch.cat([sys_user_attn_mask, asst_attn_mask], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc32a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "refcocog_dataset = datasets.load_dataset(\"jxu124/refcocog\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9be8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(refcocog_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "refcocog_dataset['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de41501",
   "metadata": {},
   "source": [
    "## Here 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor\n",
    "import torch\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "model_name = \"../checkpoints/with_policy-connector_text/checkpoint-9999/\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name, torch_dtype=torch.bfloat16,\n",
    "                                               device_map=\"cuda:3\",\n",
    "                                               attn_implementation=\"flash_attention_2\")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "path = \"/data/vlm/playground/data/coco/train2014/COCO_train2014_000000519404.jpg\"\n",
    "image = load_image(path)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "      'role': 'system',\n",
    "      'content': [\n",
    "          {'type': 'text', 'text': (f\"A conversation between a user and an assistant. You are an assistant that assists and performs the task provided by the user. \"\n",
    "                        f\"The user will provide you with a task using <task> tag that is specific to the task to be performed and an input image.\")},\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'image'},\n",
    "            {'type': 'text', 'text': \"<task:segmentation>woman in white shirt looking down at laptop computer\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "model = model.eval()\n",
    "\n",
    "prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(\"cuda:3\")\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=4000, use_cache=False, do_sample=True,)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25591ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "## extract between <seg_r> and </seg_r>\n",
    "pattern = r'<seg_r>(.*?)</seg_r>'\n",
    "matches = re.findall(pattern, generated_texts[0])\n",
    "\n",
    "## extract the float numbers from the string\n",
    "## format: <seg045> -> 0.045\n",
    "pattern = r'<seg(\\d+)>'\n",
    "matches = re.findall(pattern, matches[0])\n",
    "## if d is 3 digits, convert to float, convert to 0.d\n",
    "matches = [float(match)/1000 for match in matches]\n",
    "\n",
    "print(len(matches))\n",
    "\n",
    "## make polygon points (x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "## polygon points are in the format of (x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "answer = [16.22, 84.86, 36.76, 52.43, 69.19, 45.95, 108.11, 50.27, 118.92, 60.0, 134.05, 76.22, 132.97, 104.32, 145.95, 128.11, 151.35, 143.24, 143.78, 164.86, 139.46, 174.59, 151.35, 191.89, 189.19, 223.24, 230.27, 265.41, 235.68, 268.65, 238.92, 282.7, 232.43, 297.84, 223.78, 302.16, 183.78, 290.27, 158.92, 271.89, 105.95, 251.35, 122.16, 272.97, 137.3, 287.03, 149.19, 312.97, 162.16, 338.92, 169.73, 349.73, 174.05, 361.62, 161.08, 396.22, 149.19, 423.24, 139.46, 437.3, 125.41, 448.11, 113.51, 453.51, 104.86, 454.59, 98.38, 444.86, 78.92, 426.49, 55.14, 402.7, 40.0, 387.57, 21.62, 368.11, 1.08, 348.65, 0.0, 136.76, 0.0, 123.78]\n",
    "print(len(answer))\n",
    "\n",
    "## get the image width and height\n",
    "image_width = 640\n",
    "image_height = 480\n",
    "\n",
    "## make polygon points (x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "predicted = [(matches[i] * image_width, matches[i + 1] * image_height) for i in range(0, len(matches), 2)]\n",
    "answer = [(answer[i], answer[i + 1]) for i in range(0, len(answer), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6fbbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PreprocessorForLocalizationAndSegmentation\n",
    "\n",
    "dset = PreprocessorForLocalizationAndSegmentation.preprocess(\n",
    "    dataset_name='jxu124/refcocog',\n",
    "    split='train[:5]',\n",
    "    preprocess_fn='refcocog_sft_seg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050652f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.025, 0.177, 0.057, 0.109)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(16.22/640, 3), round(84.86 / 480, 3), round(36.76/640, 3), round(52.43/480, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4431dc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': [{'text': 'A conversation between a user and an assistant. You are an assistant that assists and performs the task provided by the user. The user will provide you with a task using <task> tag that is specific to the task to be performed and an input image.',\n",
       "     'type': 'text'}],\n",
       "   'role': 'system'},\n",
       "  {'content': [{'text': None, 'type': 'image'},\n",
       "    {'text': '<task:segmentation>two woman one in black eatting and the other has a white shirt at the desk',\n",
       "     'type': 'text'}],\n",
       "   'role': 'user'},\n",
       "  {'content': [{'text': '<seg_r><seg025><seg177><seg057><seg109><seg108><seg096><seg169><seg105><seg186><seg125><seg209><seg159><seg208><seg217><seg228><seg267><seg236><seg298><seg225><seg343><seg218><seg364><seg236><seg400><seg296><seg465><seg360><seg553><seg368><seg560><seg373><seg589><seg363><seg620><seg350><seg630><seg287><seg605><seg248><seg566><seg166><seg524><seg191><seg569><seg215><seg598><seg233><seg652><seg253><seg706><seg265><seg729><seg272><seg753><seg252><seg825><seg233><seg882><seg218><seg911><seg196><seg934><seg177><seg945><seg164><seg947><seg154><seg927><seg123><seg889><seg086><seg839><seg062><seg807><seg034><seg767><seg002><seg726><seg000><seg285><seg000><seg258></seg_r>two woman one in black eatting and the other has a white shirt at the desk',\n",
       "     'type': 'text'}],\n",
       "   'role': 'assistant'}],\n",
       " 'img_new_path': '/data/vlm/playground/data/coco/train2014/COCO_train2014_000000519404.jpg',\n",
       " 'img_width': 640,\n",
       " 'img_height': 480,\n",
       " 'task_target': '<seg_r><seg025><seg177><seg057><seg109><seg108><seg096><seg169><seg105><seg186><seg125><seg209><seg159><seg208><seg217><seg228><seg267><seg236><seg298><seg225><seg343><seg218><seg364><seg236><seg400><seg296><seg465><seg360><seg553><seg368><seg560><seg373><seg589><seg363><seg620><seg350><seg630><seg287><seg605><seg248><seg566><seg166><seg524><seg191><seg569><seg215><seg598><seg233><seg652><seg253><seg706><seg265><seg729><seg272><seg753><seg252><seg825><seg233><seg882><seg218><seg911><seg196><seg934><seg177><seg945><seg164><seg947><seg154><seg927><seg123><seg889><seg086><seg839><seg062><seg807><seg034><seg767><seg002><seg726><seg000><seg285><seg000><seg258></seg_r>two woman one in black eatting and the other has a white shirt at the desk',\n",
       " 'chosen_annot': 'two woman one in black eatting and the other has a white shirt at the desk',\n",
       " 'pil_img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=640x480>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "16.22/640, 84.86/480, 36.76/640, 52.43/480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3680af",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the points\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread(path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.scatter(*zip(*predicted), color='red', s=10)\n",
    "plt.scatter(*zip(*answer), color='blue', s=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.encode(\"<seg000>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b777d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvizr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
